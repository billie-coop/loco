<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DDAID: Documentation-Driven AI Development</title>
    <style>
        body {
            font-family: 'Courier New', monospace;
            font-size: 15px;
            line-height: 1.6;
            color: #000;
            background-color: #fff;
            max-width: 650px;
            margin: 60px auto;
            padding: 20px;
        }

        h1 {
            font-size: 28px;
            font-weight: 700;
            margin-bottom: 10px;
            letter-spacing: -0.5px;
        }

        .subtitle {
            font-size: 16px;
            color: #666;
            margin-bottom: 40px;
            font-style: italic;
        }

        h2 {
            font-size: 20px;
            font-weight: 700;
            margin-top: 50px;
            margin-bottom: 20px;
            padding: 10px;
            background-color: #000;
            color: #fff;
            display: inline-block;
        }

        h3 {
            font-size: 16px;
            font-weight: 700;
            margin-top: 30px;
            margin-bottom: 15px;
            text-decoration: underline;
        }

        p {
            margin-bottom: 20px;
        }

        blockquote {
            margin: 40px 0;
            padding: 20px;
            border: 3px solid #000;
            font-size: 16px;
            background-color: #f9f9f9;
        }

        code {
            background-color: #f5f5f5;
            padding: 2px 6px;
            font-size: 14px;
            border: 1px solid #ddd;
        }

        pre {
            background-color: #000;
            color: #fff;
            padding: 20px;
            overflow-x: auto;
            margin: 30px 0;
            font-size: 14px;
            line-height: 1.4;
        }

        a {
            color: #000;
            text-decoration: underline;
        }

        ul {
            margin: 20px 0;
            padding-left: 30px;
        }

        li {
            margin-bottom: 10px;
        }

        .box {
            border: 2px solid #000;
            padding: 20px;
            margin: 30px 0;
            background-color: #fff;
        }

        .footer {
            margin-top: 80px;
            padding-top: 30px;
            border-top: 2px solid #000;
            text-align: center;
            font-weight: 700;
        }

        @media (max-width: 700px) {
            body {
                margin: 20px;
                padding: 10px;
            }
            
            h1 {
                font-size: 24px;
            }
            
            h2 {
                font-size: 18px;
            }
        }
    </style>
</head>
<body>
    <h1>DDAID: Documentation-Driven AI Development</h1>
    <div class="subtitle">automatic context management for AI coding assistants</div>

    <p>Modern AI coding assistants already maintain context - through files like <code>CLAUDE.md</code>, project scanning, and conversation memory. The problem isn't that context doesn't exist. The problem is that context management is manual, fragmented, and doesn't scale.</p>

    <h2>The Context Management Problem</h2>
    
    <p>Current approaches to AI context have significant limitations:</p>
    
    <ul>
        <li><strong>Manual maintenance</strong> - You have to remember to update context files</li>
        <li><strong>Scattered formats</strong> - Each tool has its own way of storing context</li>
        <li><strong>Stale information</strong> - Context doesn't update when code changes</li>
        <li><strong>No cross-tool sharing</strong> - Claude can't use Copilot's understanding</li>
        <li><strong>Doesn't scale</strong> - Works for small projects, breaks down for large ones</li>
    </ul>

    <p>This leads to a familiar cycle: you carefully set up context files, they work great for a week, then slowly drift out of sync with reality until you're explaining the same things over and over again.</p>

    <h2>What DDAID Actually Does</h2>
    
    <p>DDAID takes the ad-hoc context management that already exists and makes it systematic:</p>
    
    <div class="box">
        <p><strong>Current state:</strong> Manual context files + project scanning + hope</p>
        <p><strong>DDAID approach:</strong> Automatic updates + standardized format + git integration</p>
    </div>

    <p>It's not about giving AI memory - it's about making that memory manageable.</p>

    <h2>Technical Approach</h2>
    
    <h3>Automatic Updates</h3>
    <p>When code changes, context updates. No more remembering to edit <code>ARCHITECTURE.md</code> after a refactor. Git integration detects changes and updates relevant context automatically.</p>
    
    <h3>Specialized Agents</h3>
    <p>Different aspects require different context management:</p>
    <ul>
        <li>Architecture context tracks structural decisions</li>
        <li>API context maintains endpoint documentation</li>
        <li>Security context documents auth patterns</li>
        <li>Performance context captures optimization choices</li>
    </ul>
    
    <h3>Standardized Format</h3>
    <p>One context format that works across AI tools. Your architectural decisions work the same in Claude, ChatGPT, or local models.</p>

    <h2>Why This Matters</h2>
    
    <p>The difference between theory and practice:</p>
    
    <p><strong>In theory:</strong> AI assistants read your project and understand it.</p>
    
    <p><strong>In practice:</strong> You're constantly reminding them about decisions made last month, explaining why you chose PostgreSQL over MongoDB (again), and dealing with suggestions that conflict with established patterns.</p>
    
    <p>DDAID acknowledges that context already exists - it just makes it actually work at scale.</p>

    <blockquote>
        Good context management is invisible. You only notice it when it's broken.
    </blockquote>

    <h2>Implementation Status</h2>
    
    <p>The reference implementation (Loco) demonstrates these concepts with:</p>
    
    <ul>
        <li>Git-based change detection for incremental updates</li>
        <li>Multi-tier analysis system for existing codebases</li>
        <li>Local-first design with LM Studio integration</li>
        <li>Context that travels with your repository</li>
    </ul>

    <p>It's early, but it shows how automatic context management could work in practice.</p>

    <h2>Try It</h2>
    
    <pre>git clone https://github.com/billie-coop/loco
go build && ./loco</pre>
    
    <p>See the <a href="https://github.com/billie-coop/loco">GitHub repository</a> for more details.</p>

    <div style="margin-top: 80px; border-top: 2px solid #000; padding-top: 40px;">
        <h1>Loco: Local AI Infrastructure Experiments</h1>
        <div class="subtitle">replicating cloud AI capabilities with local models</div>

        <p>While DDAID is a philosophy that any AI tool could adopt, Loco is a specific exploration: What would a local AI coding assistant look like if we tried to replicate the sophisticated patterns of cloud AI providers, but entirely offline?</p>
        
        <p>It's both a reference implementation of DDAID and a testbed for ideas about local AI orchestration. These are patterns that emerged from asking: "Without Anthropic's infrastructure, how can we still achieve powerful AI assistance?"</p>

        <h2>Progressive Multi-Tier Analysis</h2>
        
        <p>Instead of throwing one large model at everything, Loco uses a progressive approach:</p>
        
        <div class="box">
            <p><strong>Tier 1: Quick Overview (3B model, 2-3 seconds)</strong><br>
            Rapid project structure understanding, language detection, framework identification.</p>
            
            <p><strong>Tier 2: Detailed Analysis (7B model, 30s-2min)</strong><br>
            Parallel file-by-file analysis, importance scoring, dependency mapping.</p>
            
            <p><strong>Tier 3: Knowledge Synthesis (14B+ model, 2-5min)</strong><br>
            Deep architectural understanding, pattern recognition, comprehensive documentation.</p>
        </div>
        
        <p>This mirrors how cloud providers likely handle different tasks with different models, but proves it's possible locally.</p>

        <h2>Diversity of Experts</h2>
        
        <p>Rather than one model trying to understand everything, Loco explores using specialized models:</p>
        
        <ul>
            <li>Small, fast models for routine analysis</li>
            <li>Code-specialized models for understanding</li>
            <li>Larger models for synthesis and reasoning</li>
            <li>Future: Domain-specific models (security, performance, etc.)</li>
        </ul>
        
        <p>Each model does what it's best at. Together, they achieve more than any single model could.</p>

        <h2>Incremental Intelligence</h2>
        
        <p>Loco pioneered several efficiency techniques that make local AI practical:</p>
        
        <ul>
            <li><strong>Git-based caching</strong> - Only analyze files that actually changed</li>
            <li><strong>Progressive enhancement</strong> - Start with quick analysis, enhance on demand</li>
            <li><strong>Parallel processing</strong> - Multiple models analyzing different files simultaneously</li>
            <li><strong>Smart context windows</strong> - Automatic sizing based on model capabilities</li>
        </ul>
        
        <p>These aren't just optimizations - they're explorations of how to make AI assistance sustainable without cloud resources.</p>

        <h2>Local-First Philosophy</h2>
        
        <blockquote>
            "What if you could have Claude-quality assistance without sending your code anywhere?"
        </blockquote>
        
        <p>Loco experiments with:</p>
        <ul>
            <li>LM Studio integration for easy model management</li>
            <li>Automatic model selection based on task complexity</li>
            <li>Graceful degradation when large models aren't available</li>
            <li>Context that lives in your git repo, not the cloud</li>
        </ul>

        <h2>Why This Matters</h2>
        
        <p>Cloud AI providers have massive infrastructure advantages. But Loco demonstrates that with clever orchestration, local models can achieve surprisingly sophisticated results. This isn't just about privacy or offline access - it's about understanding what's possible when we rethink AI assistance from first principles.</p>
        
        <p>Every technique in Loco is an experiment: Can we replicate cloud-scale AI capabilities using only what runs on a developer's machine? The answer, increasingly, is yes.</p>

        <h2>Join the Experiment</h2>
        
        <p>Loco is open source and actively exploring new ideas:</p>
        
        <ul>
            <li>Better model orchestration strategies</li>
            <li>New specialized agents for different domains</li>
            <li>Improved caching and incremental analysis</li>
            <li>Integration with more local model runners</li>
        </ul>
        
        <p>These are just ideas being explored - not prescriptions. Fork it, modify it, take it in new directions. The goal is to push forward what's possible with local AI.</p>
        
        <p>If you're interested in local AI, model orchestration, or just want AI assistance that respects your privacy and works offline, <a href="https://github.com/billie-coop/loco">join us</a>.</p>
    </div>

    <div class="footer">
        <p style="font-size: 14px;">
            build with care • build for each other • build for your idea(l)s<br>
            build DIY • make things available to everyone • make the world a better place
        </p>
    </div>
</body>
</html>